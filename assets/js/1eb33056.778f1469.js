"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[9012],{49:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>l,frontMatter:()=>s,metadata:()=>n,toc:()=>d});const n=JSON.parse('{"id":"book-additions","title":"book-additions","description":"Rag-architecture-appendix title RAG Architecture & Tooling sidebar_label: RAG Architecture","source":"@site/docs/book-additions.md","sourceDirName":".","slug":"/book-additions","permalink":"/hackathon-practice-book/docs/book-additions","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/book-additions.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 8: Architecture","permalink":"/hackathon-practice-book/docs/architecture"}}');var r=i(4848),a=i(8453);const s={},o=void 0,c={},d=[{value:"Rag-architecture-appendix title: Appendix: RAG Architecture &amp; Tooling sidebar_label: RAG Architecture",id:"rag-architecture-appendix-title-appendix-rag-architecture--tooling-sidebar_label-rag-architecture",level:2},{value:"Appendix: The Technical Architecture",id:"appendix-the-technical-architecture",level:3},{value:"The 5-Phase Retrieval Flow",id:"the-5-phase-retrieval-flow",level:2},{value:"The RAG loop involves five distinct stages to ensure accurate, context-aware responses.",id:"the-rag-loop-involves-five-distinct-stages-to-ensure-accurate-context-aware-responses",level:3}];function h(e){const t={admonition:"admonition",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.h2,{id:"rag-architecture-appendix-title-appendix-rag-architecture--tooling-sidebar_label-rag-architecture",children:"Rag-architecture-appendix title: Appendix: RAG Architecture & Tooling sidebar_label: RAG Architecture"}),"\n",(0,r.jsx)(t.h3,{id:"appendix-the-technical-architecture",children:"Appendix: The Technical Architecture"}),"\n",(0,r.jsx)(t.p,{children:"This section documents the specific technical implementation used to build this book and its integrated intelligence systems. It serves as a reference for the Hackathon judges and a testing ground for the embedded RAG Chatbot."}),"\n",(0,r.jsx)(t.admonition,{title:"Testing the Chatbot",type:"tip",children:(0,r.jsx)(t.p,{children:'Use the definitions and steps below to test the accuracy of the Retrieval-Augmented Generation system. Ask questions like "What is the 5-phase RAG flow?" or "How does Spec-Kit Plus work?"'})}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsx)(t.li,{children:"The Retrieval-Augmented Generation (RAG) Architecture"}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"The core intelligence of this project is powered by a custom RAG pipeline connecting Docusaurus, FastAPI, and Qdrant."}),"\n",(0,r.jsx)(t.h2,{id:"the-5-phase-retrieval-flow",children:"The 5-Phase Retrieval Flow"}),"\n",(0,r.jsx)(t.h3,{id:"the-rag-loop-involves-five-distinct-stages-to-ensure-accurate-context-aware-responses",children:"The RAG loop involves five distinct stages to ensure accurate, context-aware responses."}),"\n",(0,r.jsx)(t.p,{children:"Ingestion (Preprocessing): When the book is built, Markdown content is chunked into digestible segments (paragraphs/headings). These chunks are converted into numerical embeddings using a pre-trained model."}),"\n",(0,r.jsx)(t.p,{children:'Storage (Qdrant Cloud): These vectors are stored in the Qdrant Cloud Free Tier vector database alongside metadata (chapter titles, file paths). This creates the "knowledge index."'}),"\n",(0,r.jsx)(t.p,{children:"User Query & Vectorization: When a user asks a question, the FastAPI backend receives it and converts it into a vector using the same embedding model."}),"\n",(0,r.jsx)(t.p,{children:"Retrieval: The backend queries Qdrant for the top $k$ text chunks mathematically closest to the query vector. This ensures the LLM receives context only from this book."}),"\n",(0,r.jsx)(t.p,{children:"Generation: The retrieved chunks + user query are sent to the LLM (via OpenAI Agents/ChatKit). The LLM generates the final response based strictly on the provided context."})]})}function l(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},8453:(e,t,i)=>{i.d(t,{R:()=>s,x:()=>o});var n=i(6540);const r={},a=n.createContext(r);function s(e){const t=n.useContext(a);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),n.createElement(a.Provider,{value:t},e.children)}}}]);